import mysql.connector
from mysql.connector import Error
import streamlit as st
import pandas as pd
import numpy as np
import yfinance as yf
import plotly.express as px
import plotly.graph_objects as go
from io import BytesIO
from datetime import datetime
import itertools # Th∆∞ vi·ªán ƒë·ªÉ t·∫°o c√°c c·∫∑p so s√°nh cho AHP

# --- C·∫§U H√åNH TRANG WEB ---
st.set_page_config(
    page_title="TOPSIS & AHP - Ph√¢n t√≠ch R·ªßi ro C·ªï phi·∫øu",
    page_icon="‚öñÔ∏è",
    layout="wide"
)

# --- C√ÅC BI·∫æN C·ªê ƒê·ªäNH CHO LU·ªíNG DATABASE ---
DB_CRITERIA = ['RSI', 'MACD', 'trailingPE_snapshot', 'marketCap_snapshot', 'Returns']
DB_IMPACTS = {
    'RSI': 'Cost',
    'MACD': 'Benefit',
    'trailingPE_snapshot': 'Cost',
    'marketCap_snapshot': 'Benefit',
    'Returns': 'Benefit'
}

# === DICTIONARY GI·∫¢I TH√çCH C√ÅC CH·ªà S·ªê ===
CRITERIA_EXPLANATIONS = {
    # Yahoo Finance
    'Beta': "ƒêo l∆∞·ªùng m·ª©c ƒë·ªô bi·∫øn ƒë·ªông c·ªßa c·ªï phi·∫øu so v·ªõi th·ªã tr∆∞·ªùng chung. Beta < 1 cho th·∫•y √≠t bi·∫øn ƒë·ªông h∆°n th·ªã tr∆∞·ªùng (√≠t r·ªßi ro h∆°n).",
    'P/E': "T·ª∑ l·ªá Gi√° tr√™n Thu nh·∫≠p. Ch·ªâ s·ªë n√†y cho bi·∫øt nh√† ƒë·∫ßu t∆∞ s·∫µn s√†ng tr·∫£ bao nhi√™u cho m·ªôt ƒë·ªìng l·ª£i nhu·∫≠n. P/E qu√° cao c√≥ th·ªÉ l√† d·∫•u hi·ªáu ƒë·ªãnh gi√° ƒë·∫Øt.",
    'N·ª£/V·ªën CSH': "T·ª∑ l·ªá N·ª£ tr√™n V·ªën ch·ªß s·ªü h·ªØu, ƒëo l∆∞·ªùng ƒë√≤n b·∫©y t√†i ch√≠nh. T·ª∑ l·ªá < 1 th∆∞·ªùng ƒë∆∞·ª£c coi l√† an to√†n.",
    'ROE': "L·ª£i nhu·∫≠n tr√™n V·ªën ch·ªß s·ªü h·ªØu. ƒêo l∆∞·ªùng kh·∫£ nƒÉng sinh l·ªùi c·ªßa c√¥ng ty. ROE > 15% th∆∞·ªùng ƒë∆∞·ª£c xem l√† t·ªët.",
    'Bi√™n LN': "Bi√™n L·ª£i nhu·∫≠n. Cho bi·∫øt c√¥ng ty t·∫°o ra bao nhi√™u l·ª£i nhu·∫≠n t·ª´ doanh thu. Bi√™n l·ª£i nhu·∫≠n c√†ng cao c√†ng t·ªët.",
    # Database
    'RSI': "Ch·ªâ s·ªë S·ª©c m·∫°nh T∆∞∆°ng ƒë·ªëi. ƒêo l∆∞·ªùng t·ªëc ƒë·ªô v√† s·ª± thay ƒë·ªïi c·ªßa c√°c bi·∫øn ƒë·ªông gi√°. RSI > 70 cho th·∫•y t√≠n hi·ªáu 'qu√° mua' (c√≥ th·ªÉ s·ªõm ƒëi·ªÅu ch·ªânh gi·∫£m).",
    'MACD': "ƒê∆∞·ªùng Trung b√¨nh ƒë·ªông h·ªôi t·ª•/ph√¢n k·ª≥. L√† m·ªôt ch·ªâ b√°o xu h∆∞·ªõng. MACD > 0 th∆∞·ªùng b√°o hi·ªáu xu h∆∞·ªõng tƒÉng.",
    'trailingPE_snapshot': "T∆∞∆°ng t·ª± P/E, t·ª∑ l·ªá gi√° tr√™n thu nh·∫≠p trong 12 th√°ng g·∫ßn nh·∫•t.",
    'marketCap_snapshot': "V·ªën h√≥a th·ªã tr∆∞·ªùng. C√°c c√¥ng ty c√≥ v·ªën h√≥a l·ªõn th∆∞·ªùng ·ªïn ƒë·ªãnh v√† √≠t r·ªßi ro h∆°n.",
    'Returns': "T·ª∑ su·∫•t l·ª£i nhu·∫≠n c·ªßa c·ªï phi·∫øu trong m·ªôt kho·∫£ng th·ªùi gian."
}

# --- H√ÄM K·∫æT N·ªêI DATABASE ---
@st.cache_resource
def get_db_connection():
    """T·∫°o k·∫øt n·ªëi t·ªõi MySQL database"""
    try:
        connection = mysql.connector.connect(
            host='127.0.0.1',
            user='root',
            password='130225',      # <-- THAY ƒê·ªîI M·∫¨T KH·∫®U C·ª¶A B·∫†N T·∫†I ƒê√ÇY
            database='tickets',    # <-- THAY ƒê·ªîI T√äN DATABASE C·ª¶A B·∫†N T·∫†I ƒê√ÇY
            port=3306
        )
        return connection
    except Error as e:
        st.error(f"‚ùå L·ªói k·∫øt n·ªëi database: {e}")
        return None

# --- H√ÄM L·∫§Y D·ªÆ LI·ªÜU ---
@st.cache_data
def get_stock_data(tickers_list):
    """L·∫•y c√°c ch·ªâ s·ªë t√†i ch√≠nh t·ª´ Yahoo Finance"""
    financial_data = []
    progress_bar = st.progress(0, text="ƒêang t·∫£i d·ªØ li·ªáu t·ª´ Yahoo Finance...")

    for idx, ticker_symbol in enumerate(tickers_list):
        try:
            ticker = yf.Ticker(ticker_symbol)
            info = ticker.info
            data = {
                'M√£ CP': ticker_symbol, 'Beta': info.get('beta'), 'P/E': info.get('trailingPE'),
                'N·ª£/V·ªën CSH': info.get('debtToEquity'), 'ROE': info.get('returnOnEquity'),
                'Bi√™n LN': info.get('profitMargins')
            }
            financial_data.append(data)
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Kh√¥ng th·ªÉ l·∫•y d·ªØ li·ªáu cho '{ticker_symbol}'.")
            financial_data.append({
                'M√£ CP': ticker_symbol, 'Beta': None, 'P/E': None, 'N·ª£/V·ªën CSH': None,
                'ROE': None, 'Bi√™n LN': None
            })
        progress_bar.progress((idx + 1) / len(tickers_list))

    progress_bar.empty()
    if not financial_data: return pd.DataFrame()

    df = pd.DataFrame(financial_data).set_index('M√£ CP')
    for col in df.columns:
        if df[col].isnull().all():
            st.error(f"‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu cho c·ªôt '{col}'")
            return pd.DataFrame()
        if df[col].isnull().any():
            median_val = df[col].median()
            df[col] = df[col].fillna(median_val if pd.notna(median_val) else 0)
    return df

@st.cache_data
def get_data_from_db(_conn):
    """L·∫•y d·ªØ li·ªáu t·ª´ MySQL database"""
    if _conn and _conn.is_connected():
        try:
            # <-- THAY ƒê·ªîI T√äN B·∫¢NG C·ª¶A B·∫†N T·∫†I ƒê√ÇY
            return pd.read_sql("SELECT * FROM tickets_combined", _conn)
        except Exception as e:
            st.error(f"‚ùå L·ªói truy v·∫•n database: {e}")
            return pd.DataFrame()
    return pd.DataFrame()

# === H√ÄM T√çNH TO√ÅN C·ª¶A AHP V√Ä TOPSIS ===
def calculate_ahp_weights(comparison_matrix):
    """T√≠nh tr·ªçng s·ªë (eigenvector) c·ªßa ma tr·∫≠n AHP v√† T·ª∑ l·ªá nh·∫•t qu√°n (CR)."""
    try:
        A = np.array(comparison_matrix)
        n = A.shape[0]

        eigenvalues, eigenvectors = np.linalg.eig(A)
        max_eigen_value = np.max(eigenvalues.real)
        
        max_eigen_vector_index = np.argmax(eigenvalues.real)
        weights = eigenvectors[:, max_eigen_vector_index].real
        weights = weights / weights.sum()
        
        CI = (max_eigen_value - n) / (n - 1) if n > 1 else 0
        
        RI_values = {1: 0.00, 2: 0.00, 3: 0.58, 4: 0.90, 5: 1.12, 6: 1.24, 7: 1.32, 8: 1.41, 9: 1.45, 10: 1.49}
        RI = RI_values.get(n, 1.49) 

        CR = CI / RI if RI != 0 else 0
        
        return weights, CR, pd.DataFrame(A)
    except Exception as e:
        st.error(f"L·ªói t√≠nh to√°n AHP: {e}")
        n_crit = len(comparison_matrix)
        return np.array([1/n_crit for _ in range(n_crit)]), 1.0, pd.DataFrame(np.identity(n_crit))


def run_topsis(decision_matrix, weights, impacts):
    """Thu·∫≠t to√°n TOPSIS"""
    norm_matrix = decision_matrix / np.sqrt((decision_matrix**2).sum(axis=0))
    weighted_matrix = norm_matrix * weights
    
    impacts_arr = np.array([1 if i == 'Benefit' else -1 for i in impacts])
    
    ideal_best = np.where(impacts_arr == 1, weighted_matrix.max(axis=0), weighted_matrix.min(axis=0))
    ideal_worst = np.where(impacts_arr == -1, weighted_matrix.max(axis=0), weighted_matrix.min(axis=0))

    dist_to_best = np.sqrt(((weighted_matrix - ideal_best)**2).sum(axis=1))
    dist_to_worst = np.sqrt(((weighted_matrix - ideal_worst)**2).sum(axis=1))
    
    epsilon = 1e-6
    return dist_to_worst / (dist_to_best + dist_to_worst + epsilon)

# === C√ÅC H√ÄM TI·ªÜN √çCH KH√ÅC ===
def generate_analysis_text(results_df, weights, impacts):
    """T·∫°o ra c√°c ƒëo·∫°n vƒÉn b·∫£n ph√¢n t√≠ch v√† g·ª£i √Ω"""
    if results_df.empty: return "", "", ""
    
    best_stock_name = results_df.index[0]
    worst_stock_name = results_df.index[-1]
    best_stock_data = results_df.iloc[0]
    worst_stock_data = results_df.iloc[-1]
    
    impact_map = {'Benefit': 1, 'Cost': -1}
    numerical_impacts = {k: impact_map[v] for k, v in impacts.items()}

    best_reasons = []
    for crit, impact_val in numerical_impacts.items():
        if crit not in results_df.columns: continue
        val = best_stock_data[crit]
        if (impact_val == 1 and val >= results_df[crit].quantile(0.75)) or \
           (impact_val == -1 and val <= results_df[crit].quantile(0.25)):
            best_reasons.append(f"**{crit}** ({val:.2f})")

    best_analysis = f"üèÜ **{best_stock_name}** ƒë∆∞·ª£c x·∫øp h·∫°ng cao nh·∫•t. ƒêi·ªÉm m·∫°nh ch√≠nh ƒë·∫øn t·ª´ c√°c ch·ªâ s·ªë: {', '.join(best_reasons[:3])}."
    
    worst_reasons = []
    for crit, impact_val in numerical_impacts.items():
        if crit not in results_df.columns: continue
        val = worst_stock_data[crit]
        if (impact_val == 1 and val <= results_df[crit].quantile(0.25)) or \
           (impact_val == -1 and val >= results_df[crit].quantile(0.75)):
            worst_reasons.append(f"**{crit}** ({val:.2f})")
    
    worst_analysis = f"‚ö†Ô∏è **{worst_stock_name}** c√≥ r·ªßi ro cao nh·∫•t trong danh s√°ch, ch·ªß y·∫øu do c√°c ch·ªâ s·ªë ch∆∞a t·ªët nh∆∞: {', '.join(worst_reasons[:3])}."

    actionable_advice = """
    üí° **G·ª£i √Ω h√†nh ƒë·ªông:**
    - **ƒê·ªëi v·ªõi c√°c c·ªï phi·∫øu top ƒë·∫ßu:** ƒê√¢y l√† nh·ªØng ·ª©ng vi√™n s√°ng gi√°. H√£y c√¢n nh·∫Øc ƒë∆∞a v√†o danh s√°ch theo d√µi v√† **ph√¢n t√≠ch s√¢u h∆°n** v·ªÅ y·∫øu t·ªë c∆° b·∫£n c·ªßa doanh nghi·ªáp.
    - **ƒê·ªëi v·ªõi c√°c c·ªï phi·∫øu cu·ªëi b·∫£ng:** C·∫ßn th·∫≠n tr·ªçng. N·∫øu ƒëang n·∫Øm gi·ªØ, b·∫°n n√™n **xem x√©t l·∫°i v·ªã th·∫ø** v√† c√≥ th·ªÉ ƒë·∫∑t c√°c bi·ªán ph√°p ph√≤ng ng·ª´a r·ªßi ro nh∆∞ **l·ªánh c·∫Øt l·ªó (stop-loss)**.
    - **L∆∞u √Ω:** K·∫øt qu·∫£ n√†y ho√†n to√†n d·ª±a tr√™n c√°c ch·ªâ s·ªë v√† tr·ªçng s·ªë b·∫°n ƒë√£ cung c·∫•p. ƒê√¢y l√† c√¥ng c·ª• tham kh·∫£o, kh√¥ng ph·∫£i l·ªùi khuy√™n ƒë·∫ßu t∆∞.
    """
    
    return best_analysis, worst_analysis, actionable_advice

def format_large_number(num):
    """ƒê·ªãnh d·∫°ng s·ªë l·ªõn (vd: v·ªën h√≥a) th√†nh d·∫°ng t·ª∑, tri·ªáu."""
    if pd.isna(num): return "N/A"
    num = float(num)
    if num >= 1e12: return f"{num / 1e12:.2f} ngh√¨n t·ª∑"
    if num >= 1e9: return f"{num / 1e9:.2f} t·ª∑"
    if num >= 1e6: return f"{num / 1e6:.2f} tri·ªáu"
    return f"{num:,.2f}"

# --- GIAO DI·ªÜN CH√çNH ---
st.title("‚öñÔ∏è ·ª®ng d·ª•ng Ph√¢n t√≠ch R·ªßi ro ƒê·∫ßu t∆∞ C·ªï phi·∫øu b·∫±ng TOPSIS v√† AHP")
st.info("üéØ X·∫øp h·∫°ng r·ªßi ro c·ªï phi·∫øu d·ª±a tr√™n c√°c ch·ªâ s·ªë t√†i ch√≠nh v·ªõi tr·ªçng s·ªë t√πy ch·ªânh (c√≥ th·ªÉ d√πng AHP ƒë·ªÉ t√≠nh tr·ªçng s·ªë)")

# --- THANH ƒêI·ªÄU KHI·ªÇN (SIDEBAR) ---
with st.sidebar:
    st.header("‚öôÔ∏è B·∫£ng ƒëi·ªÅu khi·ªÉn")
    data_source = st.radio("üìÇ Ch·ªçn ngu·ªìn d·ªØ li·ªáu:", ["Yahoo Finance API", "Database MySQL"], key="data_source")

    weights = {}
    CR = 1.0 
    
    if data_source == "Yahoo Finance API":
        criteria_yf = {'Beta': -1, 'P/E': -1, 'N·ª£/V·ªën CSH': -1, 'ROE': 1, 'Bi√™n LN': 1}
        criteria_list = list(criteria_yf.keys())
        n_criteria = len(criteria_list)
        tickers_input = st.text_area("Nh·∫≠p m√£ c·ªï phi·∫øu (ph√¢n c√°ch b·ªüi d·∫•u ph·∫©y):", "AAPL, MSFT, GOOGL, TSLA, NVDA")
        col_impacts = {k: ('Benefit' if v == 1 else 'Cost') for k, v in criteria_yf.items()}
    else:
        criteria_list = DB_CRITERIA
        n_criteria = len(criteria_list)
        col_impacts = DB_IMPACTS

    st.header("‚öñÔ∏è Thi·∫øt l·∫≠p Tr·ªçng s·ªë")
    
    tab_ahp, tab_manual = st.tabs(["üî¢ T√≠nh Tr·ªçng s·ªë b·∫±ng AHP", "üõ†Ô∏è Nh·∫≠p Tr·ªçng s·ªë Th·ªß c√¥ng"])

    # ----------------------------------------------------
    # TAB 1: T√çNH TR·ªåNG S·ªê B·∫∞NG AHP
    # ----------------------------------------------------
    with tab_ahp:
        with st.expander("‚öñÔ∏è So s√°nh ƒë·ªô quan tr·ªçng c·ªßa c√°c ti√™u ch√≠ (Nh·∫≠p li·ªáu)", expanded=True):
            for crit_i, crit_j in itertools.combinations(criteria_list, 2):
                impact_display_i = f" (*{col_impacts.get(crit_i, '')}*)" if data_source != "Yahoo Finance API" else ""
                impact_display_j = f" (*{col_impacts.get(crit_j, '')}*)" if data_source != "Yahoo Finance API" else ""

                comparison_value = st.select_slider(
                    f"**{crit_i}**{impact_display_i} quan tr·ªçng h∆°n **{crit_j}**{impact_display_j}?",
                    options=[1/9, 1/8, 1/7, 1/6, 1/5, 1/4, 1/3, 1/2, 1, 2, 3, 4, 5, 6, 7, 8, 9],
                    value=1.0,
                    format_func=lambda x: f"1/{round(1/x)}" if x < 1 else str(round(x)),
                    key=f"ahp_{crit_i}_{crit_j}_{data_source}",
                    help=f"So s√°nh {CRITERIA_EXPLANATIONS.get(crit_i)} vs {CRITERIA_EXPLANATIONS.get(crit_j)}"
                )
                st.session_state[f'ahp_comp_{crit_i}_{crit_j}'] = comparison_value

        comparison_matrix = [[0.0] * n_criteria for _ in range(n_criteria)]
        for i in range(n_criteria):
            for j in range(n_criteria):
                if i == j:
                    comparison_matrix[i][j] = 1.0
                elif i < j:
                    crit_i, crit_j = criteria_list[i], criteria_list[j]
                    comp_val = st.session_state.get(f'ahp_comp_{crit_i}_{crit_j}', 1.0)
                    comparison_matrix[i][j] = comp_val
                else:
                    comparison_matrix[i][j] = 1.0 / comparison_matrix[j][i]
                    
        ahp_weights_array, CR, AHP_Matrix_df = calculate_ahp_weights(comparison_matrix)
        
        st.session_state['weights_source'] = 'AHP'
        st.session_state['CR'] = CR
        st.session_state['AHP_Matrix_df'] = AHP_Matrix_df
        st.session_state['ahp_weights'] = {criteria_list[i]: ahp_weights_array[i] for i in range(n_criteria)}

        st.markdown("---")
        with st.expander("üí° Gi·∫£i th√≠ch T·ª∑ l·ªá nh·∫•t qu√°n (CR)"):
            st.info(
                """
                **CR (Consistency Ratio)** ƒëo l∆∞·ªùng m·ª©c ƒë·ªô logic v√† kh√¥ng m√¢u thu·∫´n trong c√°c so s√°nh c·∫∑p c·ªßa b·∫°n.
                - **CR ‚â§ 0.1 (T·ªët):** C√°c ƒë√°nh gi√° nh·∫•t qu√°n, k·∫øt qu·∫£ ƒë√°ng tin c·∫≠y.
                - **CR > 0.1 (C·∫ßn xem l·∫°i):** C√≥ s·ª± m√¢u thu·∫´n (VD: A > B, B > C, nh∆∞ng C > A). 
                
                B·∫°n n√™n ƒëi·ªÅu ch·ªânh l·∫°i c√°c so s√°nh ƒë·ªÉ k·∫øt qu·∫£ ƒë√°ng tin c·∫≠y h∆°n.
                """
            )

        with st.expander("üìä K·∫øt qu·∫£ AHP (Tr·ªçng s·ªë & CR)", expanded=True):
            st.markdown(f"**T·ª∑ l·ªá nh·∫•t qu√°n (CR):**")
            st.code(f"{CR:.4f}", language=None)
            if CR > 0.1:
                st.error("‚ö†Ô∏è CR > 0.1. M·ª©c ƒë·ªô nh·∫•t qu√°n th·∫•p.")
            else:
                st.success("‚úÖ CR ‚â§ 0.1. M·ª©c ƒë·ªô nh·∫•t qu√°n ch·∫•p nh·∫≠n ƒë∆∞·ª£c.")
                
            weights_df = pd.DataFrame(st.session_state['ahp_weights'].items(), columns=['Ti√™u ch√≠', 'Tr·ªçng s·ªë']).set_index('Ti√™u ch√≠')
            st.markdown("**Tr·ªçng s·ªë ƒë∆∞·ª£c t√≠nh to√°n:**")
            st.dataframe(weights_df.style.format("{:.4f}"))

        with st.expander("üìã Xem Ma tr·∫≠n So s√°nh C·∫∑p (ƒê·∫ßu v√†o AHP)"):
            AHP_Matrix_df.columns = criteria_list
            AHP_Matrix_df.index = criteria_list
            st.dataframe(AHP_Matrix_df.style.format("{:.3f}").background_gradient(cmap='Blues', axis=None), use_container_width=True)

    # ----------------------------------------------------
    # TAB 2: NH·∫¨P TR·ªåNG S·ªê TH·ª¶ C√îNG
    # ----------------------------------------------------
    with tab_manual:
        st.subheader("Nh·∫≠p Tr·ªçng s·ªë Th·ªß c√¥ng")
        
        default_weights = {k: 1.0 / n_criteria for k in criteria_list}
        
        if data_source == "Yahoo Finance API":
            preset = st.selectbox("üéöÔ∏è Ch·ªçn b·ªô tr·ªçng s·ªë m·∫´u:", ["T√πy ch·ªânh", "An to√†n", "C√¢n b·∫±ng", "TƒÉng tr∆∞·ªüng"], key="preset_yf_man")

            if preset == "An to√†n": default_weights = {'Beta': 0.3, 'P/E': 0.2, 'N·ª£/V·ªën CSH': 0.3, 'ROE': 0.1, 'Bi√™n LN': 0.1}
            elif preset == "C√¢n b·∫±ng": default_weights = {'Beta': 0.2, 'P/E': 0.2, 'N·ª£/V·ªën CSH': 0.2, 'ROE': 0.2, 'Bi√™n LN': 0.2}
            elif preset == "TƒÉng tr∆∞·ªüng": default_weights = {'Beta': 0.1, 'P/E': 0.1, 'N·ª£/V·ªën CSH': 0.1, 'ROE': 0.35, 'Bi√™n LN': 0.35}
        
        manual_weights = {}
        for crit in criteria_list:
            impact_display = f" (*{col_impacts.get(crit, '')}*)" if data_source != "Yahoo Finance API" else ""
            manual_weights[crit] = st.slider(
                f"üéØ {crit}{impact_display}", 0.0, 1.0, default_weights.get(crit, 1.0/n_criteria), 0.05, 
                key=f'w_{crit}_man_slider', help=CRITERIA_EXPLANATIONS.get(crit, "")
            )
        
        st.session_state['weights_source'] = 'Manual'
        st.session_state['manual_weights'] = manual_weights
        st.session_state['CR'] = 0.0 

    # ----------------------------------------------------
    # LOGIC CHUNG SAU KHI THI·∫æT L·∫¨P TR·ªåNG S·ªê
    # ----------------------------------------------------
    if st.session_state.get('weights_source') == 'AHP':
        weights = st.session_state.get('ahp_weights', {})
        CR = st.session_state.get('CR', 1.0)
    elif st.session_state.get('weights_source') == 'Manual':
        weights = st.session_state.get('manual_weights', {})
        CR = 0.0
    
    total_weight = sum(weights.values())
    st.markdown("---")
    st.markdown("**Tr·∫°ng th√°i Tr·ªçng s·ªë Hi·ªán t·∫°i**")
    if abs(total_weight - 1.0) > 0.01 and st.session_state.get('weights_source') == 'Manual':
        st.warning(f"‚ö†Ô∏è T·ªïng tr·ªçng s·ªë: {total_weight:.2f} (C·∫ßn = 1.0)")
    else:
        st.success(f"‚úÖ T·ªïng tr·ªçng s·ªë: {total_weight:.2f}")

# --- N√öT PH√ÇN T√çCH V√Ä HI·ªÇN TH·ªä K·∫æT QU·∫¢ ---
if st.button("üöÄ B·∫Øt ƒë·∫ßu Ph√¢n t√≠ch", use_container_width=True):
    
    CR = st.session_state.get('CR', 1.0)
    
    if CR > 0.1:
        st.error("CR > 0.1. Vui l√≤ng ƒëi·ªÅu ch·ªânh l·∫°i c√°c so s√°nh c·∫∑p trong Tab AHP ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh nh·∫•t qu√°n tr∆∞·ªõc khi ph√¢n t√≠ch.")
        st.stop()
        
    if not weights or sum(weights.values()) == 0:
        st.error("‚ùå Tr·ªçng s·ªë kh√¥ng h·ª£p l·ªá. Vui l√≤ng thi·∫øt l·∫≠p tr·ªçng s·ªë trong b·∫£ng ƒëi·ªÅu khi·ªÉn.")
        st.stop()
        
    normalized_weights = {k: v / sum(weights.values()) for k, v in weights.items()}
    
    raw_data = pd.DataFrame()
    if data_source == "Yahoo Finance API":
        tickers_list = [t.strip().upper() for t in tickers_input.split(',') if t.strip()]
        if not tickers_list:
            st.error("‚ùå Vui l√≤ng nh·∫≠p √≠t nh·∫•t m·ªôt m√£ c·ªï phi·∫øu.")
            st.stop()

        with st.spinner(f"‚è≥ ƒêang t·∫£i d·ªØ li·ªáu cho {len(tickers_list)} c·ªï phi·∫øu..."):
            raw_data = get_stock_data(tickers_list)
        
        criteria = {k: ('Benefit' if v == 1 else 'Cost') for k, v in criteria_yf.items()}
    else:
        with st.spinner("‚è≥ ƒêang k·∫øt n·ªëi v√† x·ª≠ l√Ω d·ªØ li·ªáu t·ª´ database..."):
            conn = get_db_connection()
            if not conn: st.stop()
            
            df_raw = get_data_from_db(conn)
            if df_raw.empty: st.stop()

            ticker_col = 'Ticker'
            if ticker_col not in df_raw.columns:
                st.error(f"‚ùå Kh√¥ng t√¨m th·∫•y c·ªôt '{ticker_col}' trong database.")
                st.stop()

            df_raw['Date'] = pd.to_datetime(df_raw.get('Date'), errors='coerce')
            df_grouped = df_raw.sort_values('Date', ascending=True).groupby(ticker_col).last().reset_index()
            
            missing_cols = [col for col in DB_CRITERIA if col not in df_grouped.columns]
            if missing_cols:
                st.error(f"‚ùå Database thi·∫øu c√°c c·ªôt: {', '.join(missing_cols)}")
                st.stop()

            st.success(f"‚úÖ ƒê√£ x·ª≠ l√Ω {len(df_grouped)} m√£ c·ªï phi·∫øu.")
        
        raw_data = df_grouped.set_index(ticker_col)[DB_CRITERIA].copy()
        raw_data.index.name = 'M√£ CP'

        for col in raw_data.columns:
            raw_data[col] = pd.to_numeric(raw_data[col], errors='coerce').fillna(raw_data[col].median())
        criteria = DB_IMPACTS

    if raw_data.empty:
        st.error("‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ ph√¢n t√≠ch.")
        st.stop()

    st.header("üìä D·ªØ li·ªáu ƒê·∫ßu v√†o sau khi x·ª≠ l√Ω")
    formatters = {col: "{:,.2f}" for col in raw_data.columns}
    if 'marketCap_snapshot' in raw_data.columns:
        formatters['marketCap_snapshot'] = format_large_number
    st.dataframe(raw_data.style.format(formatters).background_gradient(cmap='YlOrRd', axis=0), use_container_width=True)

    st.header("üèÜ K·∫øt qu·∫£ X·∫øp h·∫°ng TOPSIS")
    decision_matrix = raw_data[list(criteria.keys())]
    weights_list = np.array([normalized_weights[crit] for crit in criteria])
    impacts_list = list(criteria.values())

    scores = run_topsis(decision_matrix.values, weights_list, impacts_list)

    results_df = raw_data.copy()
    results_df['ƒêi·ªÉm TOPSIS'] = scores
    results_df['X·∫øp h·∫°ng'] = results_df['ƒêi·ªÉm TOPSIS'].rank(ascending=False).astype(int)
    results_df = results_df.sort_values(by='X·∫øp h·∫°ng')
    
    col1, col2, col3 = st.columns(3)
    col1.metric("ü•á T·ªët nh·∫•t (√çt r·ªßi ro nh·∫•t)", results_df.index[0], f"{results_df['ƒêi·ªÉm TOPSIS'].iloc[0]:.4f}")
    col2.metric("ü•à R·ªßi ro nh·∫•t", results_df.index[-1], f"{results_df['ƒêi·ªÉm TOPSIS'].iloc[-1]:.4f}")
    col3.metric("üìä ƒêi·ªÉm trung b√¨nh", f"{results_df['ƒêi·ªÉm TOPSIS'].mean():.4f}")
    
    result_formatters = {col: "{:,.4f}" for col in results_df.columns if pd.api.types.is_numeric_dtype(results_df[col]) and col not in ['X·∫øp h·∫°ng']}
    result_formatters['ƒêi·ªÉm TOPSIS'] = "{:.4f}"
    if 'marketCap_snapshot' in results_df.columns:
        result_formatters['marketCap_snapshot'] = format_large_number

    st.dataframe(
        results_df.style
        .apply(lambda row: ['background-color: #2E8B57; color: white; font-weight: bold' if row['X·∫øp h·∫°ng'] == 1 else '' for _ in row], axis=1)
        .background_gradient(cmap='Greens', subset=['ƒêi·ªÉm TOPSIS'])
        .format(result_formatters),
        use_container_width=True
    )
    
    st.markdown("---")
    st.subheader("üìù Ph√¢n t√≠ch v√† G·ª£i √Ω")
    best_analysis, worst_analysis, actionable_advice = generate_analysis_text(results_df, normalized_weights, criteria)
    st.markdown(best_analysis)
    st.markdown(worst_analysis)
    st.markdown(actionable_advice)
    st.markdown("---")

    tab1, tab2, tab3 = st.tabs(["üìä Bi·ªÉu ƒë·ªì So s√°nh", "üéØ Bi·ªÉu ƒë·ªì Radar", "üì• T·∫£i xu·ªëng"])
    with tab1:
        fig = px.bar(results_df, x=results_df.index, y='ƒêi·ªÉm TOPSIS', color='ƒêi·ªÉm TOPSIS',
                     color_continuous_scale='Greens', title='So s√°nh ƒêi·ªÉm TOPSIS gi·ªØa c√°c C·ªï phi·∫øu', text='ƒêi·ªÉm TOPSIS')
        fig.update_traces(texttemplate='%{text:.3f}', textposition='outside')
        fig.update_layout(xaxis_title="M√£ C·ªï phi·∫øu", yaxis_title="ƒêi·ªÉm TOPSIS", xaxis={'categoryorder':'total descending'})
        st.plotly_chart(fig, use_container_width=True)
    with tab2:
        top_n = min(5, len(results_df))
        top_stocks = results_df.head(top_n)
        norm_data = (top_stocks[list(criteria.keys())] - top_stocks[list(criteria.keys())].min()) / \
                      (top_stocks[list(criteria.keys())].max() - top_stocks[list(criteria.keys())].min())
        fig_radar = go.Figure()
        for ticker in top_stocks.index:
            fig_radar.add_trace(go.Scatterpolar(r=norm_data.loc[ticker].values, theta=list(criteria.keys()), fill='toself', name=ticker))
        fig_radar.update_layout(polar=dict(radialaxis=dict(visible=True, range=[0, 1])), showlegend=True, title=f"So s√°nh Top {top_n} C·ªï phi·∫øu (d·ªØ li·ªáu ƒë√£ chu·∫©n h√≥a)")
        st.plotly_chart(fig_radar, use_container_width=True)
    with tab3:
        output = BytesIO()
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            results_df.to_excel(writer, sheet_name='Ket_qua_TOPSIS')
            raw_data.to_excel(writer, sheet_name='Du_lieu_goc')
            pd.DataFrame({
                'Ch·ªâ s·ªë': list(normalized_weights.keys()),
                'Tr·ªçng s·ªë': list(normalized_weights.values()),
                'T√°c ƒë·ªông': [criteria[k] for k in normalized_weights.keys()]
            }).to_excel(writer, sheet_name='Cau_hinh_phan_tich', index=False)
        
        st.download_button("üìä T·∫£i b√°o c√°o chi ti·∫øt (.xlsx)", output.getvalue(),
                          f"topsis_ahp_report_{datetime.now().strftime('%Y%m%d')}.xlsx",
                          "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet", use_container_width=True)

st.markdown("---")
st.markdown("üî¨ **TOPSIS & AHP Analysis System**")